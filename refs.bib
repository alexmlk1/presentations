@book{leverage,
author = {Fumio Hayashi},
title = {Econometrics},
year = {2000},
publisher = {Princeton University Press},
pages = {21},
}

@book{cook_residuals_1983,
	title = {Residuals and Influence in Regression.},
	issn = {0006341X},
	url = {https://www.jstor.org/stable/2531125?origin=crossref},
	doi = {10.2307/2531125},
	journal = {Biometrics},
	year = {1983},
	author = {Cook, R. D. and Weisberg, S.},
	publisher = {Springer},
}

@book{pontryagin_max_princ,
	title = {The Mathematical Theory of Optimal Processes},
	year = {1962},
	author = {Pontryagin, L.S.},
	publisher = {John Wiley and Sons},
	journal = {Interscience},
}

@misc{ghorbani_data_2019,
	title = {Data {Shapley}: {Equitable} {Valuation} of {Data} for {Machine} {Learning}},
	shorttitle = {Data {Shapley}},
	url = {http://arxiv.org/abs/1904.02868},
	doi = {10.48550/arXiv.1904.02868},
	language = {en},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Ghorbani, Amirata and Zou, James},
	month = jun,
	year = {2019},
	note = {arXiv:1904.02868 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{kreer_bayesian_2025,
	title = {Bayesian {Influence} {Functions} for {Hessian}-{Free} {Data} {Attribution}},
	url = {http://arxiv.org/abs/2509.26544},
	doi = {10.48550/arXiv.2509.26544},
	urldate = {2025-12-05},
	publisher = {arXiv},
	author = {Kreer, Philipp Alexander and Wu, Wilson and Adam, Maxwell and Furman, Zach and Hoogland, Jesse},
	month = sep,
	year = {2025},
	note = {arXiv:2509.26544 [cs]},
	keywords = {Computer Science - Machine Learning},
}



@misc{bae_if_2022,
	title = {If {Influence} {Functions} are the {Answer}, {Then} {What} is the {Question}?},
	url = {http://arxiv.org/abs/2209.05364},
	doi = {10.48550/arXiv.2209.05364},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05364 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{koh_accuracy_2019,
	title = {On the {Accuracy} of {Influence} {Functions} for {Measuring} {Group} {Effects}},
	url = {http://arxiv.org/abs/1905.13289},
	doi = {10.48550/arXiv.1905.13289},
	language = {en},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Ang, Kai-Siang and Teo, Hubert H. K. and Liang, Percy},
	month = nov,
	year = {2019},
	note = {arXiv:1905.13289 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{koh_understanding_2020,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	doi = {10.48550/arXiv.1703.04730},
	language = {en},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Liang, Percy},
	month = dec,
	year = {2020},
	note = {arXiv:1703.04730 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{mu_SGD_2025,
	title = {Descend or {Rewind}? {Stochastic} {Gradient} {Descent} {Unlearning}},
	shorttitle = {Descend or {Rewind}?},
	url = {http://arxiv.org/abs/2511.15983},
	doi = {10.48550/arXiv.2511.15983},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Mu, Siqiao and Klabjan, Diego},
	month = nov,
	year = {2025},
	note = {arXiv:2511.15983 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{mu_rewind--delete_2025,
	title = {Rewind-to-{Delete}: {Certified} {Machine} {Unlearning} for {Nonconvex} {Functions}},
	shorttitle = {Rewind-to-{Delete}},
	url = {http://arxiv.org/abs/2409.09778},
	doi = {10.48550/arXiv.2409.09778},
	language = {en},
	urldate = {2025-12-01},
	publisher = {arXiv},
	author = {Mu, Siqiao and Klabjan, Diego},
	month = oct,
	year = {2025},
	note = {arXiv:2409.09778 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lee_timescales_2025,
	title = {Influence {Dynamics} and {Stagewise} {Data} {Attribution}},
	url = {http://arxiv.org/abs/2510.12071},
	doi = {10.48550/arXiv.2510.12071},
	abstract = {Current training data attribution (TDA) methods treat the influence one sample has on another as static, but neural networks learn in distinct stages that exhibit changing patterns of influence. In this work, we introduce a framework for stagewise data attribution grounded in singular learning theory. We predict that influence can change non-monotonically, including sign flips and sharp peaks at developmental transitions. We first validate these predictions analytically and empirically in a toy model, showing that dynamic shifts in influence directly map to the model’s progressive learning of a semantic hierarchy. Finally, we demonstrate these phenomena at scale in language models, where token-level influence changes align with known developmental stages.},
	language = {en},
	urldate = {2025-12-05},
	publisher = {arXiv},
	author = {Lee, Jin Hwa and Smith, Matthew and Adam, Maxwell and Hoogland, Jesse},
	month = oct,
	year = {2025},
	note = {arXiv:2510.12071 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/alex/Zotero/storage/GT5J7R8F/Lee et al. - 2025 - Influence Dynamics and Stagewise Data Attribution.pdf:application/pdf},
}

@misc{qiao_soft_2025,
	title = {Soft {Weighted} {Machine} {Unlearning}},
	url = {http://arxiv.org/abs/2505.18783},
	doi = {10.48550/arXiv.2505.18783},
	abstract = {Machine unlearning, as a post-hoc processing technique, has gained widespread adoption in addressing challenges like bias mitigation and robustness enhancement, colloquially, machine unlearning for fairness and robustness. However, existing non-privacy unlearning-based solutions persist in using binary data removal framework designed for privacy-driven motivation, leading to significant information loss, a phenomenon known as “over-unlearning”. While over-unlearning has been largely described in many studies as primarily causing utility degradation, we investigate its fundamental causes and provide deeper insights in this work through counterfactual leave-one-out analysis. In this paper, we introduce a weighted influence function that assigns tailored weights to each sample by solving a convex quadratic programming problem analytically. Building on this, we propose a soft-weighted framework enabling fine-grained model adjustments to address the over-unlearning challenge. We demonstrate that the proposed soft-weighted scheme is versatile and can be seamlessly integrated into most existing unlearning algorithms. Extensive experiments show that in fairness- and robustness-driven tasks, the soft-weighted scheme significantly outperforms hard-weighted schemes in fairness/robustness metrics and alleviates the decline in utility metric, thereby enhancing machine unlearning algorithm as an effective correction solution.},
	language = {en},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Qiao, Xinbao and Ding, Ningning and Cheng, Yushi and Zhang, Meng},
	month = may,
	year = {2025},
	note = {arXiv:2505.18783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/home/alex/Zotero/storage/V6UKH6V8/Qiao et al. - 2025 - Soft Weighted Machine Unlearning.pdf:application/pdf},
}

@article{Berthier_2024,
   title={Learning Time-Scales in Two-Layers Neural Networks},
   volume={25},
   ISSN={1615-3383},
   url={http://dx.doi.org/10.1007/s10208-024-09664-9},
   DOI={10.1007/s10208-024-09664-9},
   number={5},
   journal={Foundations of Computational Mathematics},
   publisher={Springer Science and Business Media LLC},
   author={Berthier, Raphaël and Montanari, Andrea and Zhou, Kangjie},
   year={2024},
   month=aug, pages={1627–1710} }

