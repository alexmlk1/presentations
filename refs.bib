@misc{bae_if_2022,
	title = {If Influence Functions are the Answer, Then What is the Question?},
	url = {http://arxiv.org/abs/2209.05364},
	doi = {10.48550/arXiv.2209.05364},
	abstract = {Influence functions efficiently estimate the effect of removing a single training data point on a model's learned parameters. While influence estimates align well with leave-one-out retraining for linear models, recent works have shown this alignment is often poor in neural networks. In this work, we investigate the specific factors that cause this discrepancy by decomposing it into five separate terms. We study the contributions of each term on a variety of architectures and datasets and how they vary with factors such as network width and training time. While practical influence function estimates may be a poor match to leave-one-out retraining for nonlinear networks, we show they are often a good approximation to a different object we term the proximal Bregman response function ({PBRF}). Since the {PBRF} can still be used to answer many of the questions motivating influence functions, such as identifying influential or mislabeled examples, our results suggest that current algorithms for influence function estimation give more informative results than previous error analyses would suggest.},
	number = {{arXiv}:2209.05364},
	publisher = {{arXiv}},
	author = {Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger},
	urldate = {2025-11-04},
	date = {2022-09-12},
	eprinttype = {arxiv},
	eprint = {2209.05364 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/alex/Zotero/storage/6NHB2F23/Bae et al. - 2022 - If Influence Functions are the Answer, Then What i.pdf:application/pdf;Snapshot:/home/alex/Zotero/storage/AB52HUD2/Bae et al. - 2022 - If Influence Functions are the Answer, Then What i.html:text/html},
}

@misc{koh_accuracy_2019,
	title = {On the Accuracy of Influence Functions for Measuring Group Effects},
	url = {http://arxiv.org/abs/1905.13289},
	doi = {10.48550/arXiv.1905.13289},
	abstract = {Inﬂuence functions estimate the effect of removing a training point on a model without the need to retrain. They are based on a ﬁrst-order Taylor approximation that is guaranteed to be accurate for sufﬁciently small changes to the model, and so are commonly used to study the effect of individual points in large datasets. However, we often want to study the effects of large groups of training points, e.g., to diagnose batch effects or apportion credit between different data sources. Removing such large groups can result in signiﬁcant changes to the model. Are inﬂuence functions still accurate in this setting? In this paper, we ﬁnd that across many different types of groups and for a range of real-world datasets, the predicted effect (using inﬂuence functions) of a group correlates surprisingly well with its actual effect, even if the absolute and relative errors are large. Our theoretical analysis shows that such strong correlation arises only under certain settings and need not hold in general, indicating that real-world datasets have particular properties that allow the inﬂuence approximation to be accurate.},
	number = {{arXiv}:1905.13289},
	publisher = {{arXiv}},
	author = {Koh, Pang Wei and Ang, Kai-Siang and Teo, Hubert H. K. and Liang, Percy},
	urldate = {2025-11-04},
	date = {2019-11-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.13289 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/alex/Zotero/storage/MFTTHQCI/Koh et al. - 2019 - On the Accuracy of Influence Functions for Measuring Group Effects.pdf:application/pdf},
}

@misc{koh_understanding_2020,
	title = {Understanding Black-box Predictions via Influence Functions},
	url = {http://arxiv.org/abs/1703.04730},
	doi = {10.48550/arXiv.1703.04730},
	abstract = {How can we explain the predictions of a blackbox model? In this paper, we use inﬂuence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop a simple, efﬁcient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to inﬂuence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that inﬂuence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
	number = {{arXiv}:1703.04730},
	publisher = {{arXiv}},
	author = {Koh, Pang Wei and Liang, Percy},
	urldate = {2025-11-07},
	date = {2020-12-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.04730 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/alex/Zotero/storage/8EEIW5T5/Koh and Liang - 2020 - Understanding Black-box Predictions via Influence Functions.pdf:application/pdf},
}
