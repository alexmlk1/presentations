\documentclass[%
	a4paper,%							A4 paper
	oneside,%							oneside (left and right margin are equal)  - twoside if I want to print it as a book
	%bibliography=totoc,%	add the bibliography to the table of contents
	%listof=totoc,%				add the list of figures and list of tables to the table of contents
	numbers=noenddot,%		no dot at the end of heading numbers (see the comment below)
	%headsepline,%					line after the page head
	%footsepline,%					line before the page foot
	%headings=small,%			smaller headings
	12pt,%								font size
]{scrreprt}


% input encoding
\usepackage[utf8]{inputenc}

% color in the text
\usepackage[table,xcdraw,dvipsnames]{xcolor}

% mathematical symbols
\usepackage{amsmath, amssymb, amsfonts, mathrsfs, bbm}

% hyper reference for links or figures
\usepackage{hyperref}

% package to modify itemizations and enumerations
\usepackage{enumitem}

\usepackage{tikz}

% package for better spacing
\usepackage{xspace}

% package for graphics - the formats png, pdf, jpg are permitted
\usepackage{graphicx}
\usepackage{subcaption}
% default path for figures
\graphicspath{{images/}}

% Theorems
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{definition*}{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{obs}{Observation}
\newtheorem*{ex}{Example}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\qedsymbol{$\blacksquare$}

% define useful mathematical symbols
\def\N{\ensuremath{\mathbb{N}}}
\def\R{\ensuremath{\mathbb{R}}}
\def\X{\ensuremath{\mathcal{X}}}
\def\Y{\ensuremath{\mathcal{Y}}}
\def\G{\ensuremath{\mathcal{G}}}
\def\C{\ensuremath{\mathbb{C}}}
\def\P{\ensuremath{\mathbb{P}}}
\def\Ll{\ensuremath{\mathscr{L}}}
\newcommand{\Ss}{\mathscr{S}}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Ee}{\mathbb{E}}
\newcommand{\iid}{\mathop{\sim}\limits^{\text{i.i.d.}}}
\newcommand{\qst}{\noindent\textbf{\color{Red} Question:}}
\def\eps{\ensuremath{\varepsilon}}


\begin{document}

\begin{center}
  \textbf{\LARGE Weekly meetings}
\end{center}
\chapter*{If Influence Functions are the Answer, Then What is the Question?}

\section*{Setting}

Consider a prediction task (regression problem) with:
\begin{itemize}
    \item Input space $\X$;
    \item Output space $\Y$;
    \item Training set $\mathcal{D}^n=\{z_i\}_{i=1}^n$ where $z_i=(x_i,y_i)$ for all $i=1,\dots,n$; 
    \\ $X = (x_1,...,x_n)$, $Y=(y_1,...,y_n)$;
    \item Parameter $\theta \in \Theta := \R^d$;
    \item $f(\theta; x)$ estimator of $\Y\mid \X$;
    \item Loss function $\ell:\Y\times\Y\to\R$ (e.g., 
    $\ell(y',y)\mapsto \|y'-y\|^2$).
\end{itemize}

We aim to minimize the training error (empirical risk):
\[
L(\theta;\mathcal{D}^n) 
    = \frac{1}{n}\sum_{i=1}^n \ell\!\big(f(\theta; x_i),\, y_i\big).
\]

Call 
\[
\hat{\theta} \in \arg\min_{\theta\in\Theta} L(\theta; \mathcal{D}^n).
\]

How different is it from
\[
\hat{\theta}_{\varepsilon, -z}
    = \arg\min_{\theta\in\Theta}
        \bigl( L(\theta; \mathcal{D}^n) - \varepsilon\, \ell(f(\theta; x), y)\bigr)
\]

when $\epsilon=1/n$ and $z=z_{\bar i}$ for some $i\in[n]$?\\
To answer this question, we can re-train the whole model on $\mathcal{D}^n\setminus\{z\}$ (Leave-One-Out method), or use influence functions.\\
In the context where the influence functions are well defined, they are a powerful tool. However, when applied to multi-layer perceptrons, for example, their capability of approximating the effect of the LOO decreases drastically.
This paper presents a new point of view: IFs do not approximate the LOO retraining, but instead the effect of another method they present, called PBRF.

\section*{Influence functions}

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence loss difference}
    relative to $\bar{z}$ is
    \[
    \mathcal{Q}(\bar{z};\hat\theta)
        = \left.\frac{d}{d\varepsilon}
            \left[\ell(f(\hat\theta_{\eps,-z},\bar x),\bar y)\right]
        \right|_{\varepsilon=1/n}.
    \]
\end{definition*}


\textit{Interpretation:}  
It measures how much the training error changes when the data point $\bar{z}$ is removed.

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence function}
    relative to $\bar{z}$ is
    \[
    \mathcal{I}(\bar{z};\hat\theta)
    = \lim_{\varepsilon\to 0}
        \frac{
        \hat{\theta}_{\varepsilon+1/n,-\bar{z}}
        - \hat{\theta}_{1/n,-\bar{z}}
        }{\varepsilon}.
    \]
\end{definition*}


\textit{Interpretation:}  
It represents the direction in which the optimal parameter moves when the
training objective is perturbed by removing $\bar{z}$.\\
Assuming that $L$ is strongly convex in $\theta$, we can rewrite the previous quantities in the closed forms:
\[
\mathcal{I}(z;\hat\theta)
    =  H_{\hat{\theta}}^{-1}\,
      \nabla \ell(f(\hat{\theta};x),y),
\qquad
\mathcal{Q}(z;\hat\theta)
    = \nabla \ell(f(\hat{\theta};x),y)^{\top}
      H_{\hat{\theta}}^{-1}
      \nabla \ell(f(\hat{\theta};x),y),
\]
where $H_{\hat{\theta}}$ is the Hessian of $L$ at $\hat\theta$.
\begin{remark}
  This part is not really clear: referring to the original reference \cite{cook_residuals_1983}, the derivation should be as follow, with our notation.\\
  Let $L_i(\theta) = (L(\hat\theta;\mathcal{D}^n) - 1/n \,\ell(f(\hat\theta;x_i),y_i))$. Assuming that $L$ is twice differentiable, we can use Taylor near $\hat\theta$:
  \begin{equation*}
    L_i(\theta) = L_i(\hat\theta) + (\theta-\hat\theta)^\top L_i'(\hat\theta) + \frac{1}{2}(\theta-\hat\theta)^\top L_i''(\hat\theta) (\theta-\hat\theta) + \mathrm{O}(\|\theta-\hat\theta\|^3)
  \end{equation*}
  Then, by minimizing the difference $L_i(\theta) - L_i(\hat\theta)$ for $\theta\neq\hat\theta$, we get:
  \begin{equation*}
    \theta = \hat\theta - (HL_i(\hat\theta))^{-1}JL_i(\hat\theta).
  \end{equation*}
  From this expression is also clear that the Influence Function estimator represents the parameter after one step of the Newton algorithm starting from $\hat\theta$ trying to get to $\hat\theta_{1/n,-z_i}$. 
\end{remark}

Unfortunately, there is some problems with these formulae:
\begin{enumerate}
    \item the strong convexity is essential. If at a minimum point H has any $0$-eigenvalue, we cannot invert it.
    \item Even when $L$ is strongly convex, the problem is not trivial because computing the inverse of the Hessian and the matrix-vector product are heavy computations.

\end{enumerate}

\textbf{Solution 1.}  
For iHVP, there exist efficient approximations requiring $O(nd)$ flops instead of $O(n^3)$.

\textbf{Solution 2.}  
Change point of view: Influence functions are not approximators of LOO retraining, but instead of the proximal Bregman response function (PBRF).

\section*{Response functions}

We define the response function as:
\[
\hat{r}_z(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \left( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y) \right).
\]
Observe that $\hat{r}_z(\varepsilon)=\hat{\theta}_{\varepsilon,-z}$ and
$\hat{r}_z(0)=\hat{\theta}$.

Since $\hat{r}$ is differentiable at $0$, we can expand with Taylor at first order and we get:
\[
\hat{r}_{z,\mathrm{lin}}(1/n)
  \approx \hat{\theta}
    + \frac1n H_{\hat{\theta}}^{-1}\,
      \nabla\ell(f(\hat{\theta};x),y).
\]
We require $H_{\hat\theta}$ positive definite to invert it; thus $\hat\theta$ must be a minimizer.\\
To compute influence functions for MLPs, we can approximate $H_{\hat{\theta}}$ using the
Gauss--Newton Hessian (GNH) and add damping:
\[
\mathcal{I}^{\dagger}(z;\hat\theta)
  = \left(
        J_{y,\hat{\theta}}^{\top}
        H_{\ell,\hat{\theta}}
        J_{y,\hat{\theta}}
        + \lambda I
    \right)^{-1}
    \nabla\ell(f(\hat{\theta};x),y),
\]
where $J_{y,\hat{\theta}}$ is the Jacobian of  
$F(\theta) = (f(\theta;x_1),\ldots,f(\theta;x_n))$ and $H_{\ell,\hat{\theta}}$ is the hessian of $\ell(f(\theta;x),y)$ in $\theta=\hat\theta$.\\
Note that the damped GNH is always positive definite, as long as $H_{\hat\theta}$ is SPD.\\
We can get the previous formula by linearizing the response function of the regularized loss:
\[
\begin{aligned}
\hat{r}_{z,\mathrm{damp}}(\varepsilon)
  &= \arg\min_{\theta\in\Theta}
     \Big( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\hat{\theta}\|^2 \Big),\\[4pt]
\hat{r}_{z,\mathrm{damp,lin}}(1/n)
  &\approx \hat{\theta}+\frac1n\mathcal{I}^{\dagger}(z;\hat\theta).
\end{aligned}
\]
Another issue is that, in practice, the parameter we utilise is not a minimizer of $L$. 
Thus, we want to consider a risk for which the early-stopped point $\theta^s$ is optimal:
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  = \frac1n\sum_{i=1}^n
      D_{\ell^{(i)}}\big(f(\theta;x_i), f(\theta^s;x_i)\big),
\]
with $D_{\ell^{(i)}}$ being the Bregman difference:
\[
D_{\ell^{(i)}}(y,y')
 = \ell(y,y_i)
   - \ell(y',y_i)
   - \nabla_1\ell(y',y_i)^\top(y-y').
\]
Intuitively, this quantity measures the difference between the evaluation of $\ell$ on $y$ and the first order Taylor expansion of $\ell$ around $y'$ computed on $y$.
\begin{obs}
  This quantity is always non-negative as long as $\ell$ is convex.
  To exemplify, choose $\ell(y,y')=\|y-y'\|^2/2$. This yields: 
  $$D_{\ell^{(i)}}(y,y')=\|y-y_i\|^2/2-\|y'-y_i\|^2/2-\langle\nabla \|y'-y_i\|^2/2,y-y_i\rangle=\|y-y'\|^2/2.$$
\end{obs}
\vspace{.5cm}\noindent
Consequently, we can define the \emph{Proximal Bregman Response Function} (PBRF) as:
\[
r^b_{z,\mathrm{damp}}(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \Big(
      \mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2
      \Big).
\]
The interesting property of this object is that the linearized PBRF satisfies
\[
r^b_{z,\mathrm{damp,lin}}(1/n)
  = \theta^s + \frac1n\mathcal{I}^{\dagger}(z;\theta^s).
\]

As a consequence, influence functions are \emph{not} approximating LOO retraining under the original
loss $L$. Instead, they approximate the effect of training from $\theta^s$ under the modified objective
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  - \tfrac1n\ell(f(\theta;x),y)
  + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2.
\]
This fact makes PBRF a more suitable benchmark when testing the performances of IFs.\\
Concrete examples show that actually PBRF achieves good results in tasks such as mislabeled example detection, making it a viable alternative to LOO retraining.

\section*{Error decomposition}

The approximation error of influence functions decomposes into:

\begin{itemize}
\item \textbf{Warm-start gap:} LOO starts from a random parameter (cold start), while IF are related to $\theta^s$; we can then converge to another "optimal" point;
\item \textbf{Proximity gap:} the factor $||theta-theta^s||$ induces the warm start not to move far away from $theta^s$;
\item \textbf{Non-convergence gap:} in practice we almost never start from a fully trained network;
\item \textbf{Linearization error:} produced by approximating the Taylor expansion at first order;
\item \textbf{Solver error:} inexact iHVP computation.
\end{itemize}
\begin{remark}
    The PBRF formulation eliminates the first three gaps.
\end{remark}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.9\linewidth]{Err_deco.png}
    \caption{Visual representation of the error defomposition on different datasets and models. The main focus is that the largest components are the first three.}
\end{figure}

\section*{Influence Functions vs Leverage}
For simplicity, let's consider the Ordinary Least Squares problem.\\
Let $X=(x_1|\dots|x_n)^\top\in\R^{(n\times d)}$, $y=(y_1,\dots,y_n)\in\R^n$, $e=(e_1,\dots,e_n)\in\R^n$, and $\theta\in\R^d$.
In this setting, assume the dataset is generated by $y=f(\theta^\star;x)+e=X\,\theta^\star+e$ where $e$ is the observational error. 
We can estimate $\theta^\star$ with $\hat\theta=(X^\top X)^{-1}X^\top y$. Consequently, the predicted data with our model will be $\hat{y} = X(X^\top X)^{-1}X^\top y$ and we call $P=X(X^\top X)^{-1}X^\top$, as it is an orthogonal projection on $\mathrm{ran}(X)$.
\begin{definition*}
    The \emph{leverage score} of the $i$-th sample data is $P_{ii}=x_i(X^\top X)^{-1}x_i^\top$.
\end{definition*}
This quantity describes how much the $i$-th sample data affects the $i$-th prediction of our model. The bigger it is, the more probable it is that the $i$-th sample point is an outlier.\\
Indeed, if we consider $X=JF(\theta)$ as the $X$ in our case, then the influence loss difference is approximately
\footnote{we can write $HF(\theta)=JF(\theta)^\top JF(\theta)+\sum\dots$ . If we omit the second term, we have the sought approximation. This is acceptable when the parameter is close to optimal, since the sum annihilates for optimal parameters.} 
the same as the leverage. However, the interpretation of the problem would become different.\\
In any case, an expression of interest that involves both leverage and influence functions is the following (cf. \cite{leverage}):
\begin{equation*}
  \hat\theta- \hat\theta_{1/n, -z} = \frac{(X^\top X)^{-1}x_i\hat e_i}{1-P_{ii}},
\end{equation*}
where $\hat{e}_i=y_i-x_i^\top\hat\theta$.\\
Observe that the $1-P_{ii}$ at denominator means that outliers also have high influence in the training.

\subsection*{Example: IFs for OLS}
Let's compute all the quantities we defined so far in the case of OLS with the euclidean loss.\\
We have:
\begin{itemize}
    \item Loss function: $\ell(f(\theta;x_i),y_i)=\frac{1}{2}(y_i - x_i^\top \theta)^2$;
    \item Gradient: $\nabla \ell(f(\theta;x_i),y_i) = -x_i (y_i - x_i^\top \theta)$;
    \item Hessian: $H_{\theta} = \sum_{i=1}^n x_i x_i^\top = X^\top X$;
    \item Influence function: $\mathcal{I}(z_i;\hat\theta) = (X^\top X)^{-1} x_i (y_i - x_i^\top \hat\theta)$;
    \item PBRF: $\frac12\arg\min_{\theta\in\Theta} \left( \frac{1}{n} \sum_{i=1}^n (x_i^\top (\theta - \hat\theta))^2 - \frac{1}{n} \|y_i - x_i^\top \theta\|^2 + \lambda ||\theta - \hat\theta||^2 \right)$;
           note that in this case $\theta^s=\hat\theta$ since we can compute it explicitly.
\end{itemize}

\section*{Reformulation of influence functions}
\begin{remark}
  The object similar to what we are going to discuss, which is present in the paper, is $\hat r _z(\varepsilon)$.
\end{remark}
Let $\mathcal{X},\,\mathcal{Y}$ be two measurable spaces and fix $D_n\subset(\mathcal{X}\times\mathcal{Y})^n$ such that $D_n=\left(z_i\right)_{i=1}^n$ with $z_i=(x_i,y_i)\in\mathcal{X}\times\mathcal{Y}$.\\
Given two random variables $X$ and $Y$ respectively on $\X$ and $\Y$, we are interested in studying the distribution of $Y|X$. 
To do so, we choose a Banach space $\Theta$ and a parametric function $f:\Theta\times\mathcal{X}\mapsto\mathcal{Y}$ as an estimator of such distribution.\\
In order to evaulate the estimators, we give the following definitions:
\begin{definition*}
  Given $\ell\in C^2(\mathcal{Y}\times\mathcal{Y};\R_+)$ (called \emph{loss function}), we define the \emph{empirical risk} for the estimator $f(\theta;x)$ on the dataset $D_n$ as:
  \begin{equation}
    R(\theta) := \sum_{i=1}^n\ell(f(\theta;x_i);y_i).
  \end{equation}
\end{definition*}
Given $\eps\in[0,1]$ and $j\in [n]$, we are interested in solving the minimization problem:
\begin{equation*}
  \min_{\theta\in\Theta} \sum_{i=1}^n \ell(f(\theta;x_i);y_i)+(\eps-1) \ell(f(\theta;x_j);y_j).
\end{equation*}
For simplicity, assume that for any $\eps$ the objective function has a unique minimum. We call $\theta(\eps)$ such minimum.
Let us introduce the notation:
\begin{equation*}
  \begin{aligned}
  \ell_j(\theta) &= \ell(f(\theta;x_j);y_j), \\
  R_j(\theta) &= \sum\limits_{\substack{i\in [n] \\i\neq j}}^n \ell_i(\theta).
  %\Theta(\epsilon) &= \left\{\theta\in\Theta \mid \nabla_\theta(L(\theta)+\eps g(\theta))=0\, \wedge\, \nabla_{\theta\theta}^2(L(\theta)+\eps g(\theta))>0  \right\}.
  \end{aligned}
\end{equation*}
Thus, we can rewrite:
$$ \theta(\eps) = \arg\min_{\theta\in\Theta} R_j(\theta)+\eps\ell_j(\theta).
$$
Notice that $\theta(\eps)$ symbolizes the parameter for which our model best fits the data while we change the weight of one data point in the training. 
It is particularly of interest to understand how $$\theta(1)=\theta^\star=\arg\min R(\theta)$$ and $$\theta(0)=\theta^\star_{-j}=\arg\min R_j(\theta)$$ are different, since they represent respectively the optimal parameter obtained while training on the whole data and on the dataset minus one particular point.\\
One way to analyze this, is by the means of influence functions.
\begin{definition*}
  The \emph{influence function} of $z_j$ is:
  \begin{equation}
    I(j):= \left.\frac{d}{d\eps}\theta(\eps)\right|_{\eps=0}=\dot\theta(0).
  \end{equation}
\end{definition*}
\begin{remark}
  The previous quantity is well defined because $\theta(\eps)$ is $C^1$ thanks to the Implicit function theorem (applied to $\nabla\mathscr{L}(\theta,\eps)=L(\theta)+\eps \ell_j(\theta)$ we get that there exists $\theta(\eps)$ differentiable such that $\nabla\mathscr{L}(\theta(\eps),\eps)=0$ in a neighbourhood of $1$ at least, since by definition $\nabla\mathscr{L}(\theta^\star,1) =\nabla R(\theta^\star)=0$).\\
  It is not clear yet if we are more interested in $\dot\theta(0)$ or $\dot\theta(1)$.
\end{remark}
\begin{proposition}
  We can write $I(j)$ explicitly as:
  \begin{equation}
    I(j) = -H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j})
  \end{equation}
  where $H_{R_j}$ is the Hessian of $R_j$ in $\theta^\star_{-j}$.
\end{proposition}
\begin{proof}
  By definition, $\theta(\eps)$ satisfies:
  \begin{equation*}
    \nabla_\theta(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0.
  \end{equation*}
  Taking another derivative in $\eps$ yields:
  \begin{eqnarray}
    \nabla^2_{\theta,\eps}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))& =0 \iff \\
    \nabla^2_{\theta\theta}R_j(\theta(\eps))\dot\theta(\eps) + \nabla_\theta \ell_j(\theta(\eps)) + \eps \nabla^2_{\theta\theta}\ell_j(\theta(\eps))\dot\theta(\eps) &=0 \iff \\
    \dot\theta(\eps) = -(\nabla^2_{\theta\theta}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps))))^{-1}\nabla_\theta \ell_j(\theta(\eps)).
  \end{eqnarray}
  Evaluating in $\eps = 0$ concludes the proof:
  \begin{equation*}
    \dot\theta(0) = -(\nabla^2_{\theta\theta}R_j(\theta(0)))^{-1} \nabla_\theta \ell_j(\theta(0)).
  \end{equation*}
\end{proof}

\begin{remark}
  If we wanted to compute $\dot\theta(1)$, it would also have a nice form:
  $$ \dot\theta(1) = -H_R^{-1} \nabla_\theta \ell_j(\theta^\star).
  $$
  where $H_R$ is the Hessian of $R$ in $\theta^\star$. Indeed, if our goal is to \emph{unlearn} a data point, this formulation does not require us to retrain the model, as we have already access to $\theta^\star$ (but not to $\theta^\star_{-j}$).
\end{remark}
It could also be of interest to consider:
\begin{equation}
  Q(z):= \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0}.
\end{equation}
\begin{corollary}
  The following formulae hold:
  \begin{equation}
    \begin{aligned}
    \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0} &= \ell_j(\theta^\star_{-j}), \\
    \left.\frac{d}{d\eps} (\ell_j(\theta(\eps)))\right|_{\eps=0} &= - \nabla_\theta \ell_j(\theta^\star_{-j})^\top H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j}),
    \end{aligned}   
  \end{equation}
\end{corollary}
\begin{proof}
  For the first equality, taking the derivative yields:
  $$ \nabla_\theta R_j(\theta(\eps))\dot\theta(\eps) + \ell_j(\theta(\eps)) + \eps\nabla_\theta \ell_j(\theta(\eps))\dot\theta(\eps)
  $$
  and recalling that on $\theta(\eps)$ it holds $\nabla(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0$ we can conclude.\\
  For the second one it suffices to substitute the definition of $\dot\theta$ from the previous equation after taking the derivative.

\end{proof}

\begin{remark}
  As discussed in \cite{koh_accuracy_2019}, it is also possible to consider $\boldsymbol{\epsilon}=(\eps_1,\dots,\eps_n)\in[0,1]^n$ and $R_{\boldsymbol{\epsilon}}(\theta)=\sum_{i=1}^n \eps_i \ell_i(\theta)$.
  Note also that in this case, their notation figures ``$\theta(0)$'', but indeed in our notation it would be $\theta(1)$ (which makes sense since we are considering a group of points).\\
  In such work, they provide formal statements about when the influence function ap\-proxima\-tions are accurate, taking into account also the difference between the influence function estimation and the \emph{Newton estimation}.
\end{remark}

\begin{definition*}
  We call Newton estimation the quantity:

  $$ I_{Nt}(j) = -(HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1))$$.
\end{definition*}
The name for this stems from the fact that this formula estimates the parameter after one step of Newton method while minimizing $\nabla R_j$ (equivalently, we are considering the second order Taylor expansion of $R_j$ in $\theta(1)$):
$$ \theta_{Nt} = \theta(1) - (HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1)).$$ 
Unfortunately, $I(j)$ and $I_{Nt}(j)$ are close when the smallest eigenvalue of the hessian is big, which is not always the case.
One way to ensure this difference is small, is to add the regularization term $\lambda\|\theta\|$ in the risk. This way, we can decrease the aforementioned error by choosing a large $\lambda$.

When working with more than one index, it makes sense to define the following quantity:
\begin{equation*}
  \theta^j(\eps) = \theta\underbrace{(1,\dots,\eps,\dots,1)}_{\text{$j$-th place}}.
\end{equation*}

\qst Is there a link between $\dot{\theta^j}$ and $J\theta$?

\section*{Gradient Descent case}
Let us specialize the analysis to the instance where we are using the gradient descent algorithm to pursue the minimization task.
Furthermore, consider the gradient flow dynamics:
\begin{equation}\tag{GF}\label{eq:GF}
  \begin{cases}
    \frac{d}{d\,t} \theta(t,\eps) = - \nabla R_j(\theta(t,\eps)) - \eps \nabla \ell_j(\theta(t,\eps))\\
    \theta(0,\eps) = \theta_0 \qquad \forall\eps\in[0,1]
  \end{cases},
\end{equation}
where we consider fixed $j\in[n]$ and $\theta_0\in\Theta$.

Let us call $\xi(t_0,t_1,\eps;\theta_0)$ the flux of \eqref{eq:GF},i.e., $\theta(t,\eps)=\xi(0,t,\eps;\theta_0)$. 
By the differential equation theory, since we are assuming $\ell\in C^2$, we know that at least $\xi\in C_t^2\times C_\eps^0$ (even Lipschitz in $\eps$).
Therefore, $\theta:[0,+\infty]\times[0,1]\to\Theta$ can be seen as a homotopy between the learning trajectories in the parameters' space (taking as definition $\theta(+\infty, \eps):=\theta(\eps)$).

\begin{figure}[hbt]
\centering
\begin{tikzpicture}[scale=2]

% Square vertices
\coordinate (A) at (0,0);   % bottom-left
\coordinate (B) at (0,2);   % top-left
\coordinate (C) at (2,2);   % top-right
\coordinate (D) at (2,0);   % bottom-right

% Draw square
\draw (A) -- (B) -- (C) -- (D) -- cycle;

% Corner labels
\node[left]  at (A) {$0$};
\node[left]  at (B) {$1$};
\node[right] at (C) {$\theta^\star_{-j}$};
\node[right] at (D) {$\theta^\star$};

% Side labels
\draw (A) -- node[left,pos=0.5] {$\theta_0$} (B);
\draw (C) -- node[right,pos=0.5] {$\theta(\eps)$} (D);
\node[right, below]  at (D) {$\infty$};

\end{tikzpicture}
\caption{Visual representation of the homotopy $\theta(t,\eps)$ where on the horizontal axis we have evolution in time and on vertical axis the change of parameter $\eps$.}
\end{figure}

\qst How different really is $\dot \theta(1)$ from $\dot \theta(0)$? 

Assuming $\theta(\eps)$ is $C^2$, we can give the bound:
$$ \dot\theta(1)-\dot\theta(0) = \int_0^1 \ddot\theta(\eps)d\,\eps \le \|\ddot\theta\|_{\infty,[0,1]},$$
but its computation requires a third derivative of $R_j$...\\
I would like to do some experiments in a simple setting for visualizing $\theta(\eps)$. This may give insights.\\
Some experiments are summarized in \autoref{fig:3}. What I found interesting about them is that the distance from the target function does not affect the linearity of the trajectory.
In fact, in the figure you can see that the first considered point is close to the real value, but the trajectory of its parameter is not a rect, unlike the second point (which is more distant).
\begin{figure}[hbt]
    \centering

    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_1_data.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_1_traj_1comp.png}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_2_data.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_2_traj_1comp.png}
    \end{subfigure}
    \caption{I used a 3-neuron 1-hidden layer MLP trained on 20 points generated by $y_i=f_*(x_i)+x_i$ where $f_*$ is another MLP of the same type and $\xi_i\sim\mathcal{N}(0,0.1)$. 
    On the left column we can observe the conisdered data point we are deleting in red, in blue the other data points and in green the target function.
    On the right side, I represented the trajectory of the first neuron as a function of $\eps$ (I trained the model for 1000 iterations, it should be enough)}
    \label{fig:3}
  \end{figure}

\qst Consider now the map $\theta(t,s):[0,+\infty]\times[0,T]$, with $T\in(0,+\infty]$ fixed a priori, such that:
\begin{equation*}
  \begin{cases}
    \theta(t,s) = \xi(0,t,1;\theta_0) & t\in[0,s]\\
    \theta(t,s) = \xi(s,t,0;\xi(0,s,1;\theta_0)) & t\in[s,T]
  \end{cases}.
\end{equation*}
What can we say about $\theta(T,s)$?

If $T=\infty$, then for any $s<\infty$ it holds that $\theta(\infty,s)=\theta^\star_{-j}$, but for finite $T$ the answer is not clear.

{\color{RoyalBlue}
\section*{Epsilon dynamic}
Note that we can rewrite $\theta(t.s)$ by transforming $\eps$ itself in a variable of time:
\begin{equation*}
  \theta(t,s)=\xi(0,t,\eps_s;\theta(0))~~ \text{ where }~~ 
  \eps_s(t)= \mathbbm{1}_{[0,s]}(t)\ .
\end{equation*}
From this formula, a natural question arises: is a step function really the best choice for $\eps(t)$?

To reply, we first need to analyse the dynamics in the very simple case of linear regression with squared loss to see if we can get any insight.\\
With this goal in mind, let's rewrite explicitly \ref{eq:GF} in this specific case (I will use the same notation as the IF vs Leverage part):
\begin{equation}\label{eq:LRGF}\tag{LRGF}
  \begin{cases}
    \frac{d}{d\,t}\theta(t,1) = -\nabla \frac12(X\theta-y)^2=-X^\top X\theta+X^\top y \\
    \theta(0)=\theta_0
  \end{cases}.
\end{equation}
This is a linear ODE, so we can find an explicit solution, which reads:
\begin{equation*}
  \theta(t,1)=\theta_0 e^{-H t}+(\mathrm{Id}_d-e^{-H t})\hat\theta .
\end{equation*}
In order to extend this to the case of every other $\eps\in(0,1)$ 
\footnote{for $\eps=0$ consider for simplicity $X',y'$ obtained by removing the target datapoint, otherwise we can integrate that case in the general one using the Moore-Penrose pseudoinverse}, 
consider \\$y^j_\eps = (y_1,\dots,\eps y_j,\dots,y_n)$ and similarly $X^j_\eps$ and $H^j_\eps=(X^j_\eps)^\top X^j_\eps$.
Solving \eqref{eq:LRGF} with the above quantities defines:
\begin{equation*}
  \theta(t,\eps)=\theta_0 e^{-H^j_\eps t}+(\mathrm{Id}_d-e^{-H^j_\eps t})\hat\theta_{-j}(\eps) .
\end{equation*}
With $s,T$ fixed, what we want to study is what is the "fastest route" to $\hat\theta_{-j}(\eps)$ when we change $\eps_s(t)$.
In other words, we are trying to solve:
\begin{equation*}
  \min_{\eps\in E_s}\|\theta(T,\eps)-\theta^\star_{-j}\|_\Theta
\end{equation*}
for $E_s=\{f\in[0,1]^{[0,+\infty]} \mid f(t)=1~\forall t\leq s \wedge f(t)=0 ~\forall t\geq T\}$.\\
For our baseline (i.e., $\eps_s(t)=\mathbbm{1}_{[0,s]}(t)$), this quantity is:
\begin{equation*}
  \|(\theta_0 e^{-H s}+(\mathrm{Id}_d-e^{-H s})\theta^\star-\theta^\star_{-j})e^{-H_{-j}(T-s)} \|.
\end{equation*}
Can we do any better?

\begin{obs}
  By the properties of $\eps\in E_s$, we can state the problem we are trying to solve as:
  \begin{equation*}
    \min_{\eps\in E_s}\|\theta(T,\eps)-\theta(\infty,\eps)\|_\Theta = \min_{\eps\in E_s}\left\|\int_{T}^\infty \partial_t\theta(t,\eps(t))\,dt\right\|_\Theta ,
  \end{equation*}
  since $d\theta=\partial_t\theta\,dt + \partial_\eps \theta\,d\eps$, but $\eps(T)=\eps(\infty)=0$.
\end{obs}

Unfortunately, as soon as we add the time dependancy of $\eps$ in the equation:
\begin{equation*}
  \begin{cases}
    \dot \theta(t,\eps(t)) = -\sum_{i\neq j} x_i(x_i^\top\theta-y_i) - \eps(t)x_j(x_j^\top\theta-y_j) \\
    \theta(0) = \theta_0
  \end{cases} ,
\end{equation*}
we cannot obtain a closed form solution anymore. In fact, even if we rewrite the first line as:
\begin{equation*}
  \begin{aligned}
    \dot \theta(t,\eps(t)) &= a(t)\theta(t) + b(t), \text{ where } \\
    a(t) &= -\sum_{i\neq j}x_ix_i^\top - \eps(t)x_jx_j^\top \\
    b(t) &= \sum_{i\neq j}x_iy_i + \eps(t)x_jy_j
  \end{aligned}
\end{equation*}
and use the method of the Wronskian with the variation of constants, that does not yield a closed-form solution\footnote{This method requires $\eps\in C^1$ because to apply the Wronskian we need to take another derivative in the homogeneous equation and consider $\ddot\theta-\dot\theta a(t) - \theta \dot a(t)$.}.\\
Another method that may produce some more interesting results is the application of optimal control theory.
Namely, we can apply the Pontryagin Maximum Principle for linear equations with variable coefficients (cf. \cite{pontryagin_max_princ} Theorem 15 p.184):
\begin{theorem}[PMP]
  Consider a system of differential equations of the form:
  \begin{equation*}
    \frac{d\,x^i}{d\,t} = \sum_{\nu=1}^{n}a_\nu^i (t) x^\nu + \sum_{\rho=1}^{r} b_\rho(t) u^\rho + f^i(t) \quad i=1,\dots,n~.
  \end{equation*}
  Call $A=(a_\nu^i)_{\nu,i=1}^n \in \R^{n \times n}$, $B = (b_\rho)_{\rho=1}^r \in \R^{n \times r}$. 
  If $A^{\otimes i} B$ are linearly independent for every $i$ and if $A(t),B(t)$ are at least $n$ times differentiable, then
  there exists a unique solution $u(t)=(u^1,\dots,u^r)\in U$ such that the flow $x(t)$ gets from $x(t_0)=x_0$ to $x(t_1)=x_1$ in minimum time.
  Moreover, $u$ is piecewise constant and its values are vertices or the polyhedron $U$.
\end{theorem}
We can apply this to our case for $x^i=\theta_i$, $u^j=\eps$; it follows that the optimal $\eps$ is piecewise constant and can only take on values $0$ or $1$.\\
In particular, I think it shouldn't be too difficult to prove that it is constantly $0$, but direct computations may be heavy.\\
In addition, note that we do not make any assumptions (except sufficient regularity) on $a(t),b(t),f(t)$. Therefore, this theorem is applicable also for the general case with $C^\infty$ activation functions.
}

\section*{Algorithm implementations: D2D vs R2D}
An application for which we may want to use influence functions is unlearning a data point (for example for privacy reasons).
However, as we have seen, IFs may not work well in the MLP and Deep Learning setting. 
Therefore, in the literature some algorithms have been developed to unlearn training points without using the highly inefficient LOO retraining.
For example, in \cite{mu_rewind--delete_2025}, the authors take inspiration from the \emph{Descend-to-Delete} algorithm (D2D) and create a more efficient version called \emph{Rewind-to-Delete} (R2D).

The former algorithm is very intuitive and consists on simply continuing the training of the model but on the updated dataset where we deleted the target data point.
By continuing for enough iteration, there have been proven some theoretical guarantees that the final parameter distribution is in some sense undistinguishable from the one of LOO method.\\
The problem with this algorithm is that such guarantees only hold in the convex case and cannot be extended to the more general setting (e.g. ReLU and tanh functions are not convex, so this result does not hold).

On the other hand, the R2D algorithm saves a check-point parameter during the training (let's say after the K-th iteration) and then keeps going until iteration T.
The idea is that when we receive the request of unlearning the target point, we will continue the training on the updated dataset starting from $\theta(K)$, instead of $\theta(T)$.\\
The authors of the paper show that also this method yields indistinguishalbility, but in this case the result holds also for non-convex functions and the iterations required are much less than the total training time $T$ (required by LOO).

In \cite{mu_SGD_2025}, the same researchers extend the previous results also to the (projected) SGD case.
In particular, in addition to all previous observations, they highlight that D2D provides tighter bounds for the undistinguishability due to its reliance on the convergence to a unique global minimum, while R2D has more loose estimates as it only counts on the underlying contractivity of gradient systems.

\section*{Other interesting things we might want to explore}
\begin{itemize}
  \item If our goal is to gain a better understanding of what data points are the most informative during the training, instead of trying to unlearn certain data, the results in \cite{ghorbani_data_2019} might be interesting.
  In their paper, the authors prove that their Shapley values-based method performs better on this task rather than influence function methods. 
  \item It may be of interest to study Bayesian Influence Functions (BIFs) as well as frequentistic ones. 
  In \cite{kreer_bayesian_2025}, the researchers present an unlearning method that uses BIFs instead of IFs. The reason for this is we don't need to compute the iHVP to evaluate BIFs, therefore this method works better with more singular loss landscapes.
  As another consequence, we don't need to evaluate these quantities on local minima (we dont need the hessian to be invertible), which is one of the less realistic hypotheses for IFs.\\
  On the other hand, computations are not always faster than IFs (here, the leading cost is estimating the covariance between two elements) and achieving good results requires more hyperparameter tuning than classical methods.
  {\color{RoyalBlue}
  \item Paper about machine unlearning with $\eps\neq0$ \cite{qiao_soft_2025} (not for privacy issues, but, for example, to not take into account data with wrong labels);
  \item Something I had not seen yet in machine unlearning, but I have seen for example in plateau explanation \cite{Berthier_2024}: it can make more sense to consider influence functions in certain epochs of the training, not only at the end \cite{lee_timescales_2025}. 
  Indeed, the importance of a certain data point may change throughout the training.
  }
\end{itemize}


\nocite{*}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}