\documentclass[%
	a4paper,%							A4 paper
	oneside,%							oneside (left and right margin are equal)  - twoside if I want to print it as a book
	%bibliography=totoc,%	add the bibliography to the table of contents
	%listof=totoc,%				add the list of figures and list of tables to the table of contents
	numbers=noenddot,%		no dot at the end of heading numbers (see the comment below)
	%headsepline,%					line after the page head
	%footsepline,%					line before the page foot
	%headings=small,%			smaller headings
	12pt,%								font size
]{scrreprt}


% input encoding
\usepackage[utf8]{inputenc}

% color in the text
\usepackage[table,xcdraw,dvipsnames]{xcolor}

% mathematical symbols
\usepackage{amsmath, amssymb, amsfonts, mathrsfs}

% hyper reference for links or figures
\usepackage{hyperref}

% package to modify itemizations and enumerations
\usepackage{enumitem}

% package for better spacing
\usepackage{xspace}

% package for graphics - the formats png, pdf, jpg are permitted
\usepackage{graphicx}
% default path for figures
\graphicspath{{images/}}

% Theorems
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{definition*}{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{obs}{Observation}
\newtheorem*{ex}{Example}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand\qedsymbol{$\blacksquare$}

% define useful mathematical symbols
\def\N{\ensuremath{\mathbb{N}}}
\def\R{\ensuremath{\mathbb{R}}}
\def\X{\ensuremath{\mathcal{X}}}
\def\Y{\ensuremath{\mathcal{Y}}}
\def\G{\ensuremath{\mathcal{G}}}
\def\C{\ensuremath{\mathbb{C}}}
\def\P{\ensuremath{\mathbb{P}}}
\def\Ll{\ensuremath{\mathscr{L}}}
\newcommand{\Ss}{\mathscr{S}}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Ee}{\mathbb{E}}
\newcommand{\iid}{\mathop{\sim}\limits^{\text{i.i.d.}}}
\def\eps{\ensuremath{\varepsilon}}


\begin{document}

\begin{center}
  \textbf{\LARGE Weekly meetings}
\end{center}
\chapter*{If Influence Functions are the Answer, Then What is the Question?}

\section*{Setting}

Consider a prediction task (regression problem) with:
\begin{itemize}
    \item Input space $\X$;
    \item Output space $\Y$;
    \item Training set $\mathcal{D}^n=\{z_i\}_{i=1}^n$ where $z_i=(x_i,y_i)$ for all $i=1,\dots,n$; 
    \\ $X = (x_1,...,x_n)$, $Y=(y_1,...,y_n)$;
    \item Parameter $\theta \in \Theta := \R^d$;
    \item $f(\theta; x)$ estimator of $\Y\mid \X$;
    \item Loss function $\ell:\Y\times\Y\to\R$ (e.g., 
    $\ell(y',y)\mapsto \|y'-y\|^2$).
\end{itemize}

We aim to minimize the training error (empirical risk):
\[
L(\theta;\mathcal{D}^n) 
    = \frac{1}{n}\sum_{i=1}^n \ell\!\big(f(\theta; x_i),\, y_i\big).
\]

Call 
\[
\hat{\theta} \in \arg\min_{\theta\in\Theta} L(\theta; \mathcal{D}^n).
\]

How different is it from
\[
\hat{\theta}_{\varepsilon, -z}
    = \arg\min_{\theta\in\Theta}
        \bigl( L(\theta; \mathcal{D}^n) - \varepsilon\, \ell(f(\theta; x), y)\bigr)
\]

when $\epsilon=1/n$ and $z=z_{\bar i}$ for some $i\in[n]$?\\
To answer this question, we can re-train the whole model on $\mathcal{D}^n\setminus\{z\}$ (Leave-One-Out method), or use influence functions.\\
In the context where the influence functions are well defined, they are a powerful tool. However, when applied to multi-layer perceptrons, for example, their capability of approximating the effect of the LOO decreases drastically.
This paper presents a new point of view: IFs do not approximate the LOO retraining, but instead the effect of another method they present, called PBRF.

\section*{Influence functions}

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence loss difference}
    relative to $\bar{z}$ is
    \[
    \mathcal{Q}(\bar{z};\hat\theta)
        = \left.\frac{d}{d\varepsilon}
            \left[\ell(f(\hat\theta_{\eps,-z},\bar x),\bar y)\right]
        \right|_{\varepsilon=1/n}.
    \]
\end{definition*}


\textit{Interpretation:}  
It measures how much the training error changes when the data point $\bar{z}$ is removed.

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence function}
    relative to $\bar{z}$ is
    \[
    \mathcal{I}(\bar{z};\hat\theta)
    = \lim_{\varepsilon\to 0}
        \frac{
        \hat{\theta}_{\varepsilon+1/n,-\bar{z}}
        - \hat{\theta}_{1/n,-\bar{z}}
        }{\varepsilon}.
    \]
\end{definition*}


\textit{Interpretation:}  
It represents the direction in which the optimal parameter moves when the
training objective is perturbed by removing $\bar{z}$.\\
Assuming that $L$ is strongly convex in $\theta$, we can rewrite the previous quantities in the closed forms:
\[
\mathcal{I}(z;\hat\theta)
    =  H_{\hat{\theta}}^{-1}\,
      \nabla \ell(f(\hat{\theta};x),y),
\qquad
\mathcal{Q}(z;\hat\theta)
    = \nabla \ell(f(\hat{\theta};x),y)^{\top}
      H_{\hat{\theta}}^{-1}
      \nabla \ell(f(\hat{\theta};x),y),
\]
where $H_{\hat{\theta}}$ is the Hessian of $L$ at $\hat\theta$.
\begin{remark}
  This part is not really clear: referring to the original reference \cite{cook_residuals_1983}, the derivation should be as follow, with our notation.\\
  Let $L_i(\theta) = (L(\hat\theta;\mathcal{D}^n) - 1/n \,\ell(f(\hat\theta;x_i),y_i))$. Assuming that $L$ is twice differentiable, we can use Taylor near $\hat\theta$:
  \begin{equation*}
    L_i(\theta) = L_i(\hat\theta) + (\theta-\hat\theta)^\top L_i'(\hat\theta) + \frac{1}{2}(\theta-\hat\theta)^\top L_i''(\hat\theta) (\theta-\hat\theta) + \mathrm{O}(\|\theta-\hat\theta\|^3)
  \end{equation*}
  Then, by minimizing the difference $L_i(\theta) - L_i(\hat\theta)$ for $\theta\neq\hat\theta$, we get:
  \begin{equation*}
    \theta = \hat\theta - (HL_i(\hat\theta))^{-1}JL_i(\hat\theta).
  \end{equation*}
  From this expression is also clear that the Influence Function estimator represents the parameter after one step of the Newton algorithm starting from $\hat\theta$ trying to get to $\hat\theta_{1/n,-z_i}$. 
\end{remark}

Unfortunately, there is some problems with these formulae:
\begin{enumerate}
    \item the strong convexity is essential. If at a minimum point H has any $0$-eigenvalue, we cannot invert it.
    \item Even when $L$ is strongly convex, the problem is not trivial because computing the inverse of the Hessian and the matrix-vector product are heavy computations.

\end{enumerate}

\textbf{Solution 1.}  
For iHVP, there exist efficient approximations requiring $O(nd)$ flops instead of $O(n^3)$.

\textbf{Solution 2.}  
Change point of view: Influence functions are not approximators of LOO retraining, but instead of the proximal Bregman response function (PBRF).

\section*{Response functions}

We define the response function as:
\[
\hat{r}_z(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \left( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y) \right).
\]
Observe that $\hat{r}_z(\varepsilon)=\hat{\theta}_{\varepsilon,-z}$ and
$\hat{r}_z(0)=\hat{\theta}$.

Since $\hat{r}$ is differentiable at $0$, we can expand with Taylor at first order and we get:
\[
\hat{r}_{z,\mathrm{lin}}(1/n)
  \approx \hat{\theta}
    + \frac1n H_{\hat{\theta}}^{-1}\,
      \nabla\ell(f(\hat{\theta};x),y).
\]
We require $H_{\hat\theta}$ positive definite to invert it; thus $\hat\theta$ must be a minimizer.\\
To compute influence functions for MLPs, we can approximate $H_{\hat{\theta}}$ using the
Gauss--Newton Hessian (GNH) and add damping:
\[
\mathcal{I}^{\dagger}(z;\hat\theta)
  = \left(
        J_{y,\hat{\theta}}^{\top}
        H_{\ell,\hat{\theta}}
        J_{y,\hat{\theta}}
        + \lambda I
    \right)^{-1}
    \nabla\ell(f(\hat{\theta};x),y),
\]
where $J_{y,\hat{\theta}}$ is the Jacobian of  
$F(\theta) = (f(\theta;x_1),\ldots,f(\theta;x_n))$ and $H_{\ell,\hat{\theta}}$ is the hessian of $\ell(f(\theta;x),y)$ in $\theta=\hat\theta$.\\
Note that the damped GNH is always positive definite, as long as $H_{\hat\theta}$ is SPD.\\
We can get the previous formula by linearizing the response function of the regularized loss:
\[
\begin{aligned}
\hat{r}_{z,\mathrm{damp}}(\varepsilon)
  &= \arg\min_{\theta\in\Theta}
     \Big( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\hat{\theta}\|^2 \Big),\\[4pt]
\hat{r}_{z,\mathrm{damp,lin}}(1/n)
  &\approx \hat{\theta}+\frac1n\mathcal{I}^{\dagger}(z;\hat\theta).
\end{aligned}
\]
Another issue is that, in practice, the parameter we utilise is not a minimizer of $L$. 
Thus, we want to consider a risk for which the early-stopped point $\theta^s$ is optimal:
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  = \frac1n\sum_{i=1}^n
      D_{\ell^{(i)}}\big(f(\theta;x_i), f(\theta^s;x_i)\big),
\]
with $D_{\ell^{(i)}}$ being the Bregman difference:
\[
D_{\ell^{(i)}}(y,y')
 = \ell(y,y_i)
   - \ell(y',y_i)
   - \nabla_1\ell(y',y_i)^\top(y-y').
\]
Intuitively, this quantity measures the difference between the evaluation of $\ell$ on $y$ and the first order Taylor expansion of $\ell$ around $y'$ computed on $y$.
\begin{obs}
  This quantity is always non-negative as long as $\ell$ is convex.
  To exemplify, choose $\ell(y,y')=\|y-y'\|^2/2$. This yields: 
  $$D_{\ell^{(i)}}(y,y')=\|y-y_i\|^2/2-\|y'-y_i\|^2/2-\langle\nabla \|y'-y_i\|^2/2,y-y_i\rangle=\|y-y'\|^2/2.$$
\end{obs}
\vspace{.5cm}\noindent
Consequently, we can define the \emph{Proximal Bregman Response Function} (PBRF) as:
\[
r^b_{z,\mathrm{damp}}(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \Big(
      \mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2
      \Big).
\]
The interesting property of this object is that the linearized PBRF satisfies
\[
r^b_{z,\mathrm{damp,lin}}(1/n)
  = \theta^s + \frac1n\mathcal{I}^{\dagger}(z;\theta^s).
\]

As a consequence, influence functions are \emph{not} approximating LOO retraining under the original
loss $L$. Instead, they approximate the effect of training from $\theta^s$ under the modified objective
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  - \tfrac1n\ell(f(\theta;x),y)
  + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2.
\]
This fact makes PBRF a more suitable benchmark when testing the performances of IFs.\\
Concrete examples show that actually PBRF achieves good results in tasks such as mislabeled example detection, making it a viable alternative to LOO retraining.

\section*{Error decomposition}

The approximation error of influence functions decomposes into:

\begin{itemize}
\item \textbf{Warm-start gap:} LOO starts from a random parameter (cold start), while IF are related to $\theta^s$; we can then converge to another "optimal" point;
\item \textbf{Proximity gap:} the factor $||theta-theta^s||$ induces the warm start not to move far away from $theta^s$;
\item \textbf{Non-convergence gap:} in practice we almost never start from a fully trained network;
\item \textbf{Linearization error:} produced by approximating the Taylor expansion at first order;
\item \textbf{Solver error:} inexact iHVP computation.
\end{itemize}
\begin{remark}
    The PBRF formulation eliminates the first three gaps.
\end{remark}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.9\linewidth]{Err_deco.png}
    \caption{Visual representation of the error defomposition on different datasets and models. The main focus is that the largest components are the first three.}
\end{figure}

\section*{Influence Functions vs Leverage}
For simplicity, let's consider the Ordinary Least Squares problem.\\
Let $X=(x_1|\dots|x_n)^\top\in\R^{(n\times d)}$, $y=(y_1,\dots,y_n)\in\R^n$, $e=(e_1,\dots,e_n)\in\R^n$, and $\theta\in\R^d$.
In this setting, assume the dataset is generated by $y=f(\theta^\star;x)+e=X\,\theta^\star+e$ where $e$ is the observational error. 
We can estimate $\theta^\star$ with $\hat\theta=(X^\top X)^{-1}X^\top y$. Consequently, the predicted data with our model will be $\hat{y} = X(X^\top X)^{-1}X^\top y$ and we call $P=X(X^\top X)^{-1}X^\top$, as it is an orthogonal projection on $\mathrm{ran}(X)$.
\begin{definition*}
    The \emph{leverage score} of the $i$-th sample data is $P_{ii}=x_i(X^\top X)^{-1}x_i^\top$.
\end{definition*}
This quantity describes how much the $i$-th sample data affects the $i$-th prediction of our model. The bigger it is, the more probable it is that the $i$-th sample point is an outlier.\\
Indeed, if we consider $X=JF(\theta)$ as the $X$ in our case, then the influence loss difference is approximately
\footnote{we can write $HF(\theta)=JF(\theta)^\top JF(\theta)+\sum\dots$ . If we omit the second term, we have the sought approximation. This is acceptable when the parameter is close to optimal, since the sum annihilates for optimal parameters.} 
the same as the leverage. However, the interpretation of the problem would become different.\\
In any case, an expression of interest that involves both leverage and influence functions is the following (cf. \cite{leverage}):
\begin{equation*}
  \hat\theta- \hat\theta_{1/n, -z} = \frac{(X^\top X)^{-1}x_i\hat e_i}{1-P_{ii}},
\end{equation*}
where $\hat{e}_i=y_i-x_i^\top\hat\theta$.\\
Observe that the $1-P_{ii}$ at denominator means that outliers also have high influence in the training.

\subsection*{Example: IFs for OLS}
Let's compute all the quantities we defined so far in the case of OLS with the euclidean loss.\\
We have:
\begin{itemize}
    \item Loss function: $\ell(f(\theta;x_i),y_i)=\frac{1}{2}(y_i - x_i^\top \theta)^2$;
    \item Gradient: $\nabla \ell(f(\theta;x_i),y_i) = -x_i (y_i - x_i^\top \theta)$;
    \item Hessian: $H_{\theta} = \sum_{i=1}^n x_i x_i^\top = X^\top X$;
    \item Influence function: $\mathcal{I}(z_i;\hat\theta) = (X^\top X)^{-1} x_i (y_i - x_i^\top \hat\theta)$;
    \item PBRF: $\frac12\arg\min_{\theta\in\Theta} \left( \frac{1}{n} \sum_{i=1}^n (x_i^\top (\theta - \hat\theta))^2 - \frac{1}{n} \|y_i - x_i^\top \theta\|^2 + \lambda ||\theta - \hat\theta||^2 \right)$;
           note that in this case $\theta^s=\hat\theta$ since we can compute it explicitly.
\end{itemize}

\section*{Reformulation of influence functions}
\begin{remark}
  The object similar to what we are going to discuss, which is present in the paper, is $\hat r _z(\varepsilon)$.
\end{remark}
Let $\mathcal{X},\,\mathcal{Y}$ be two measurable spaces and fix $D_n\subset(\mathcal{X}\times\mathcal{Y})^n$ such that $D_n=\left(z_i\right)_{i=1}^n$ with $z_i=(x_i,y_i)\in\mathcal{X}\times\mathcal{Y}$.\\
Given two random variables $X$ and $Y$ respectively on $\X$ and $\Y$, we are interested in studying the distribution of $Y|X$. 
To do so, we choose a Banach space $\Theta$ and a parametric function $f:\Theta\times\mathcal{X}\mapsto\mathcal{Y}$ as an estimator of such distribution.\\
In order to evaulate the estimators, we give the following definitions:
\begin{definition*}
  Given $\ell\in C^2(\mathcal{Y}\times\mathcal{Y};\R_+)$ (called \emph{loss function}), we define the \emph{empirical risk} for the estimator $f(\theta;x)$ on the dataset $D_n$ as:
  \begin{equation}
    R(\theta) := \sum_{i=1}^n\ell(f(\theta;x_i);y_i).
  \end{equation}
\end{definition*}
{\color{RoyalBlue}
Given $\eps\in[0,1]$ and $j\in [n]$, we are interested in solving the minimization problem:
\begin{equation*}
  \min_{\theta\in\Theta} \sum_{i=1}^n \ell(f(\theta;x_i);y_i)+(\eps-1) \ell(f(\theta;x_j);y_j).
\end{equation*}
For simplicity, assume that for any $\eps$ the objective function has a unique minimum. We call $\theta(\eps)$ such minimum.
Let us introduce the notation:
\begin{equation*}
  \begin{aligned}
  \ell_j(\theta) &= \ell(f(\theta;x_j);y_j), \\
  R_j(\theta) &= \sum\limits_{\substack{i\in [n] \\i\neq j}}^n \ell_i(\theta).
  %\Theta(\epsilon) &= \left\{\theta\in\Theta \mid \nabla_\theta(L(\theta)+\eps g(\theta))=0\, \wedge\, \nabla_{\theta\theta}^2(L(\theta)+\eps g(\theta))>0  \right\}.
  \end{aligned}
\end{equation*}
Thus, we can rewrite:
$$ \theta(\eps) = \arg\min_{\theta\in\Theta} R_j(\theta)+\eps\ell_j(\theta).
$$
Notice that $\theta(\eps)$ symbolizes the parameter for which our model best fits the data while we change the weight of one data point in the training. 
It is particularly of interest to understand how $$\theta(1)=\theta^\star=\arg\min R(\theta)$$ and $$\theta(0)=\theta^\star_{-j}=\arg\min R_j(\theta)$$ are different, since they represent respectively the optimal parameter obtained while training on the whole data and on the dataset minus one particular point.\\
}
One way to analyze this, is by the means of influence functions.
\begin{definition*}
  The \emph{influence function} of $z_j$ is:
  \begin{equation}
    I(j):= \left.\frac{d}{d\eps}\theta(\eps)\right|_{\eps=0}=\dot\theta(0).
  \end{equation}
\end{definition*}
\begin{remark}
  The previous quantity is well defined because $\theta(\eps)$ is $C^1$ thanks to the Implicit function theorem (applied to $\nabla\mathscr{L}(\theta,\eps)=L(\theta)+\eps \ell_j(\theta)$ we get that there exists $\theta(\eps)$ differentiable such that $\nabla\mathscr{L}(\theta(\eps),\eps)=0$ in a neighbourhood of $1$ at least, since by definition $\nabla\mathscr{L}(\theta^\star,1) =\nabla R(\theta^\star)=0$).\\
  It is not clear yet if we are more interested in $\dot\theta(0)$ or $\dot\theta(1)$.
\end{remark}
\begin{proposition}
  We can write $I(j)$ explicitly as:
  \begin{equation}
    I(j) = -H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j})
  \end{equation}
  where $H_{R_j}$ is the Hessian of $R_j$ in $\theta^\star_{-j}$.
\end{proposition}
\begin{proof}
  By definition, $\theta(\eps)$ satisfies:
  \begin{equation*}
    \nabla_\theta(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0.
  \end{equation*}
  Taking another derivative in $\eps$ yields:
  \begin{eqnarray*}
    \nabla^2_{\theta,\eps}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))& =0 \iff \\
    \nabla^2_{\theta\theta}R_j(\theta(\eps))\dot\theta(\eps) + \nabla_\theta \ell_j(\theta(\eps)) + \eps \nabla^2_{\theta\theta}\ell_j(\theta(\eps))\dot\theta(\eps) &=0 \iff \\
    \dot\theta(\eps) = -(\nabla^2_{\theta\theta}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps))))^{-1}\nabla_\theta \ell_j(\theta(\eps)).
  \end{eqnarray*}
  Evaluating in $\eps = 0$ concludes the proof:
  \begin{equation*}
    \dot\theta(0) = -(\nabla^2_{\theta\theta}R_j(\theta(0)))^{-1} \nabla_\theta \ell_j(\theta(0)).
  \end{equation*}
\end{proof}

\begin{remark}
  If we wanted to compute $\dot\theta(1)$, it would also have a nice form:
  $$ \dot\theta(1) = -H_R^{-1} \nabla_\theta \ell_j(\theta^\star).
  $$
  where $H_R$ is the Hessian of $R$ in $\theta^\star$. {\color{RoyalBlue} Indeed, if our goal is to \emph{unlearn} a data point, this formulation does not require us to retrain the model, as we have already access to $\theta^\star$ (but not to $\theta^\star_{-j}$). }
\end{remark}
It could also be of interest to consider:
\begin{equation}
  Q(z):= \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0}.
\end{equation}
\begin{corollary}
  The following formulae hold:
  \begin{equation}
    \begin{aligned}
    \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0} &= \ell_j(\theta^\star_{-j}), \\
    \left.\frac{d}{d\eps} (\ell_j(\theta(\eps)))\right|_{\eps=0} &= - \nabla_\theta \ell_j(\theta^\star_{-j})^\top H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j}),
    \end{aligned}   
  \end{equation}
\end{corollary}
\begin{proof}
  For the first equality, taking the derivative yields:
  $$ \nabla_\theta R_j(\theta(\eps))\dot\theta(\eps) + \ell_j(\theta(\eps)) + \eps\nabla_\theta \ell_j(\theta(\eps))\dot\theta(\eps)
  $$
  and recalling that on $\theta(\eps)$ it holds $\nabla(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0$ we can conclude.\\
  For the second one it suffices to substitute the definition of $\dot\theta$ from the previous equation after taking the derivative.

\end{proof}



{\color{RoyalBlue} 
\begin{remark}
  As discussed in \cite{koh_accuracy_2019}, it is also possible to consider $\boldsymbol{\epsilon}=(\eps_1,\dots,\eps_n)\in[0,1]^n$ and $R_{\boldsymbol{\epsilon}}(\theta)=\sum_{i=1}^n \eps_i \ell_i(\theta)$.
  Note also that in this case, their notation figures ``$\theta(0)$'', but indeed in our notation it would be $\theta(1)$ (which makes sense since we are considering a group of points).\\
  In such work, they provide formal statements about when the influence function approximations are accurate, taking into account also the difference between the influence function estimation and the \emph{Newton estimation}.
\end{remark}

\begin{definition*}
  We call Newton estimation the quantity:

  $$ I_{Nt}(j) = -(HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1))$$.
\end{definition*}
The name for this stems from the fact that this formula estimates the parameter after one step of Newton method while minimizing $\nabla R_j$ (equivalently, we are considering the second order Taylor expansion of $R_j$ in $\theta(1)$):
$$ \theta_{Nt} = \theta(1) - (HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1)).$$ 
Unfortunately, $I(j)$ and $I_{nt}(j)$ are close when the smallest eigenvalue of the hessian is big, which is not always the case.
One way to ensure this difference is small, is to add the regularization term $\lambda\|\theta\|$ in the risk. This way, we can decrease the aforementioned error by choosing a large $\lambda$.
}

\nocite{*}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}