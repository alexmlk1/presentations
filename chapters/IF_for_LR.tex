\section*{Influence Functions vs Leverage}
For simplicity, let's consider the Ordinary Least Squares problem.\\
Let $X=(x_1|\dots|x_n)^\top\in\R^{(n\times d)}$, $y=(y_1,\dots,y_n)\in\R^n$, $e=(e_1,\dots,e_n)\in\R^n$, and $\theta\in\R^d$.
In this setting, assume the dataset is generated by $y=f(\theta^\star;x)+e=X\,\theta^\star+e$ where $e$ is the observational error. 
We can estimate $\theta^\star$ with $\hat\theta=(X^\top X)^{-1}X^\top y$. Consequently, the predicted data with our model will be $\hat{y} = X(X^\top X)^{-1}X^\top y$ and we call $P=X(X^\top X)^{-1}X^\top$, as it is an orthogonal projection on $\mathrm{ran}(X)$.
\begin{definition*}
    The \emph{leverage score} of the $i$-th sample data is $P_{ii}=x_i(X^\top X)^{-1}x_i^\top$.
\end{definition*}
This quantity describes how much the $i$-th sample data affects the $i$-th prediction of our model. The bigger it is, the more probable it is that the $i$-th sample point is an outlier.\\
Indeed, if we consider $X=JF(\theta)$ as the $X$ in our case, then the influence loss difference is approximately
\footnote{we can write $HF(\theta)=JF(\theta)^\top JF(\theta)+\sum\dots$ . If we omit the second term, we have the sought approximation. This is acceptable when the parameter is close to optimal, since the sum annihilates for optimal parameters.} 
the same as the leverage. However, the interpretation of the problem would become different.\\
In any case, an expression of interest that involves both leverage and influence functions is the following (cf. \cite{leverage}):
\begin{equation*}
  \hat\theta- \hat\theta_{1/n, -z} = \frac{(X^\top X)^{-1}x_i\hat e_i}{1-P_{ii}},
\end{equation*}
where $\hat{e}_i=y_i-x_i^\top\hat\theta$.\\
Observe that the $1-P_{ii}$ at denominator means that outliers also have high influence in the training.

\subsection*{Example: IFs for OLS}
Let's compute all the quantities we defined so far in the case of OLS with the euclidean loss.\\
We have:
\begin{itemize}
    \item Loss function: $\ell(f(\theta;x_i),y_i)=\frac{1}{2}(y_i - x_i^\top \theta)^2$;
    \item Gradient: $\nabla \ell(f(\theta;x_i),y_i) = -x_i (y_i - x_i^\top \theta)$;
    \item Hessian: $H_{\theta} = \sum_{i=1}^n x_i x_i^\top = X^\top X$;
    \item Influence function: $\mathcal{I}(z_i;\hat\theta) = (X^\top X)^{-1} x_i (y_i - x_i^\top \hat\theta)$;
    \item PBRF: $\frac12\arg\min_{\theta\in\Theta} \left( \frac{1}{n} \sum_{i=1}^n (x_i^\top (\theta - \hat\theta))^2 - \frac{1}{n} \|y_i - x_i^\top \theta\|^2 + \lambda ||\theta - \hat\theta||^2 \right)$;
           note that in this case $\theta^s=\hat\theta$ since we can compute it explicitly.
\end{itemize}

\section*{Reformulation of influence functions}
\begin{remark}
  The object similar to what we are going to discuss, which is present in the paper, is $\hat r _z(\varepsilon)$.
\end{remark}
Let $\mathcal{X},\,\mathcal{Y}$ be two measurable spaces and fix $D_n\subset(\mathcal{X}\times\mathcal{Y})^n$ such that $D_n=\left(z_i\right)_{i=1}^n$ with $z_i=(x_i,y_i)\in\mathcal{X}\times\mathcal{Y}$.\\
Given two random variables $X$ and $Y$ respectively on $\X$ and $\Y$, we are interested in studying the distribution of $Y|X$. 
To do so, we choose a Banach space $\Theta$ and a parametric function $f:\Theta\times\mathcal{X}\mapsto\mathcal{Y}$ as an estimator of such distribution.\\
In order to evaulate the estimators, we give the following definitions:
\begin{definition*}
  Given $\ell\in C^2(\mathcal{Y}\times\mathcal{Y};\R_+)$ (called \emph{loss function}), we define the \emph{empirical risk} for the estimator $f(\theta;x)$ on the dataset $D_n$ as:
  \begin{equation}
    R(\theta) := \sum_{i=1}^n\ell(f(\theta;x_i);y_i).
  \end{equation}
\end{definition*}
Given $\eps\in[0,1]$ and $j\in [n]$, we are interested in solving the minimization problem:
\begin{equation*}
  \min_{\theta\in\Theta} \sum_{i=1}^n \ell(f(\theta;x_i);y_i)+(\eps-1) \ell(f(\theta;x_j);y_j).
\end{equation*}
For simplicity, assume that for any $\eps$ the objective function has a unique minimum. We call $\theta(\eps)$ such minimum.
Let us introduce the notation:
\begin{equation*}
  \begin{aligned}
  \ell_j(\theta) &= \ell(f(\theta;x_j);y_j), \\
  R_j(\theta) &= \sum\limits_{\substack{i\in [n] \\i\neq j}}^n \ell_i(\theta).
  %\Theta(\epsilon) &= \left\{\theta\in\Theta \mid \nabla_\theta(L(\theta)+\eps g(\theta))=0\, \wedge\, \nabla_{\theta\theta}^2(L(\theta)+\eps g(\theta))>0  \right\}.
  \end{aligned}
\end{equation*}
Thus, we can rewrite:
$$ \theta(\eps) = \arg\min_{\theta\in\Theta} R_j(\theta)+\eps\ell_j(\theta).
$$
Notice that $\theta(\eps)$ symbolizes the parameter for which our model best fits the data while we change the weight of one data point in the training. 
It is particularly of interest to understand how $$\theta(1)=\theta^\star=\arg\min R(\theta)$$ and $$\theta(0)=\theta^\star_{-j}=\arg\min R_j(\theta)$$ are different, since they represent respectively the optimal parameter obtained while training on the whole data and on the dataset minus one particular point.\\
One way to analyze this, is by the means of influence functions.
\begin{definition*}
  The \emph{influence function} of $z_j$ is:
  \begin{equation}
    I(j):= \left.\frac{d}{d\eps}\theta(\eps)\right|_{\eps=0}=\dot\theta(0).
  \end{equation}
\end{definition*}
\begin{remark}
  The previous quantity is well defined because $\theta(\eps)$ is $C^1$ thanks to the Implicit function theorem (applied to $\nabla\mathscr{L}(\theta,\eps)=L(\theta)+\eps \ell_j(\theta)$ we get that there exists $\theta(\eps)$ differentiable such that $\nabla\mathscr{L}(\theta(\eps),\eps)=0$ in a neighbourhood of $1$ at least, since by definition $\nabla\mathscr{L}(\theta^\star,1) =\nabla R(\theta^\star)=0$).\\
  It is not clear yet if we are more interested in $\dot\theta(0)$ or $\dot\theta(1)$.
\end{remark}
\begin{proposition}
  We can write $I(j)$ explicitly as:
  \begin{equation}
    I(j) = -H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j})
  \end{equation}
  where $H_{R_j}$ is the Hessian of $R_j$ in $\theta^\star_{-j}$.
\end{proposition}
\begin{proof}
  By definition, $\theta(\eps)$ satisfies:
  \begin{equation*}
    \nabla_\theta(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0.
  \end{equation*}
  Taking another derivative in $\eps$ yields:
  \begin{eqnarray}
    \nabla^2_{\theta,\eps}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))& =0 \iff \\
    \nabla^2_{\theta\theta}R_j(\theta(\eps))\dot\theta(\eps) + \nabla_\theta \ell_j(\theta(\eps)) + \eps \nabla^2_{\theta\theta}\ell_j(\theta(\eps))\dot\theta(\eps) &=0 \iff \\
    \dot\theta(\eps) = -(\nabla^2_{\theta\theta}(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps))))^{-1}\nabla_\theta \ell_j(\theta(\eps)).
  \end{eqnarray}
  Evaluating in $\eps = 0$ concludes the proof:
  \begin{equation*}
    \dot\theta(0) = -(\nabla^2_{\theta\theta}R_j(\theta(0)))^{-1} \nabla_\theta \ell_j(\theta(0)).
  \end{equation*}
\end{proof}

\begin{remark}
  If we wanted to compute $\dot\theta(1)$, it would also have a nice form:
  $$ \dot\theta(1) = -H_R^{-1} \nabla_\theta \ell_j(\theta^\star).
  $$
  where $H_R$ is the Hessian of $R$ in $\theta^\star$. Indeed, if our goal is to \emph{unlearn} a data point, this formulation does not require us to retrain the model, as we have already access to $\theta^\star$ (but not to $\theta^\star_{-j}$).
\end{remark}
It could also be of interest to consider:
\begin{equation}
  Q(z):= \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0}.
\end{equation}
\begin{corollary}
  The following formulae hold:
  \begin{equation}
    \begin{aligned}
    \left.\frac{d}{d\eps} (R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))\right|_{\eps=0} &= \ell_j(\theta^\star_{-j}), \\
    \left.\frac{d}{d\eps} (\ell_j(\theta(\eps)))\right|_{\eps=0} &= - \nabla_\theta \ell_j(\theta^\star_{-j})^\top H_{R_j}^{-1} \nabla_\theta \ell_j(\theta^\star_{-j}),
    \end{aligned}   
  \end{equation}
\end{corollary}
\begin{proof}
  For the first equality, taking the derivative yields:
  $$ \nabla_\theta R_j(\theta(\eps))\dot\theta(\eps) + \ell_j(\theta(\eps)) + \eps\nabla_\theta \ell_j(\theta(\eps))\dot\theta(\eps)
  $$
  and recalling that on $\theta(\eps)$ it holds $\nabla(R_j(\theta(\eps))+\eps \ell_j(\theta(\eps)))=0$ we can conclude.\\
  For the second one it suffices to substitute the definition of $\dot\theta$ from the previous equation after taking the derivative.

\end{proof}

\begin{remark}
  As discussed in \cite{koh_accuracy_2019}, it is also possible to consider $\boldsymbol{\epsilon}=(\eps_1,\dots,\eps_n)\in[0,1]^n$ and $R_{\boldsymbol{\epsilon}}(\theta)=\sum_{i=1}^n \eps_i \ell_i(\theta)$.
  Note also that in this case, their notation figures ``$\theta(0)$'', but indeed in our notation it would be $\theta(1)$ (which makes sense since we are considering a group of points).\\
  In such work, they provide formal statements about when the influence function ap\-proxima\-tions are accurate, taking into account also the difference between the influence function estimation and the \emph{Newton estimation}.
\end{remark}

\begin{definition*}
  We call Newton estimation the quantity:

  $$ I_{Nt}(j) = -(HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1))$$.
\end{definition*}
The name for this stems from the fact that this formula estimates the parameter after one step of Newton method while minimizing $\nabla R_j$ (equivalently, we are considering the second order Taylor expansion of $R_j$ in $\theta(1)$):
$$ \theta_{Nt} = \theta(1) - (HR_j(\theta(1)))^{-1}\nabla R_j(\theta(1)).$$ 
Unfortunately, $I(j)$ and $I_{Nt}(j)$ are close when the smallest eigenvalue of the hessian is big, which is not always the case.
One way to ensure this difference is small, is to add the regularization term $\lambda\|\theta\|$ in the risk. This way, we can decrease the aforementioned error by choosing a large $\lambda$.

When working with more than one index, it makes sense to define the following quantity:
\begin{equation*}
  \theta^j(\eps) = \theta\underbrace{(1,\dots,\eps,\dots,1)}_{\text{$j$-th place}}.
\end{equation*}

\qst Is there a link between $\dot{\theta^j}$ and $J\theta$?

\section*{Gradient Descent case}
Let us specialize the analysis to the instance where we are using the gradient descent algorithm to pursue the minimization task.
Furthermore, consider the gradient flow dynamics:
\begin{equation}\tag{GF}\label{eq:GF}
  \begin{cases}
    \frac{d}{d\,t} \theta(t,\eps) = - \nabla R_j(\theta(t,\eps)) - \eps \nabla \ell_j(\theta(t,\eps))\\
    \theta(0,\eps) = \theta_0 \qquad \forall\eps\in[0,1]
  \end{cases},
\end{equation}
where we consider fixed $j\in[n]$ and $\theta_0\in\Theta$.

Let us call $\xi(t_0,t_1,\eps;\theta_0)$ the flux of \eqref{eq:GF},i.e., $\theta(t,\eps)=\xi(0,t,\eps;\theta_0)$. 
By the differential equation theory, since we are assuming $\ell\in C^2$, we know that at least $\xi\in C_t^2\times C_\eps^0$ (even Lipschitz in $\eps$).
Therefore, $\theta:[0,+\infty]\times[0,1]\to\Theta$ can be seen as a homotopy between the learning trajectories in the parameters' space (taking as definition $\theta(+\infty, \eps):=\theta(\eps)$).

\begin{figure}[hbt]
\centering
\begin{tikzpicture}[scale=2]

% Square vertices
\coordinate (A) at (0,0);   % bottom-left
\coordinate (B) at (0,2);   % top-left
\coordinate (C) at (2,2);   % top-right
\coordinate (D) at (2,0);   % bottom-right

% Draw square
\draw (A) -- (B) -- (C) -- (D) -- cycle;

% Corner labels
\node[left]  at (A) {$0$};
\node[left]  at (B) {$1$};
\node[right] at (C) {$\theta^\star_{-j}$};
\node[right] at (D) {$\theta^\star$};

% Side labels
\draw (A) -- node[left,pos=0.5] {$\theta_0$} (B);
\draw (C) -- node[right,pos=0.5] {$\theta(\eps)$} (D);
\node[right, below]  at (D) {$\infty$};

\end{tikzpicture}
\caption{Visual representation of the homotopy $\theta(t,\eps)$ where on the horizontal axis we have evolution in time and on vertical axis the change of parameter $\eps$.}
\end{figure}

\qst How different really is $\dot \theta(1)$ from $\dot \theta(0)$? 

Assuming $\theta(\eps)$ is $C^2$, we can give the bound:
$$ \dot\theta(1)-\dot\theta(0) = \int_0^1 \ddot\theta(\eps)d\,\eps \le \|\ddot\theta\|_{\infty,[0,1]},$$
but its computation requires a third derivative of $R_j$...\\
I would like to do some experiments in a simple setting for visualizing $\theta(\eps)$. This may give insights.\\
Some experiments are summarized in \autoref{fig:3}. What I found interesting about them is that the distance from the target function does not affect the linearity of the trajectory.
In fact, in the figure you can see that the first considered point is close to the real value, but the trajectory of its parameter is not a rect, unlike the second point (which is more distant).
\begin{figure}[hbt]
    \centering

    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_1_data.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_1_traj_1comp.png}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_2_data.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ex_2_traj_1comp.png}
    \end{subfigure}
    \caption{I used a 3-neuron 1-hidden layer MLP trained on 20 points generated by $y_i=f_*(x_i)+x_i$ where $f_*$ is another MLP of the same type and $\xi_i\sim\mathcal{N}(0,0.1)$. 
    On the left column we can observe the conisdered data point we are deleting in red, in blue the other data points and in green the target function.
    On the right side, I represented the trajectory of the first neuron as a function of $\eps$ (I trained the model for 1000 iterations, it should be enough)}
    \label{fig:3}
  \end{figure}

\qst Consider now the map $\theta(t,s):[0,+\infty]\times[0,T]$, with $T\in(0,+\infty]$ fixed a priori, such that:
\begin{equation}\label{eq:theta_ts}
  \begin{cases}
    \theta(t,s) = \xi(0,t,1;\theta_0) & t\in[0,s]\\
    \theta(t,s) = \xi(s,t,0;\xi(0,s,1;\theta_0)) & t\in[s,T]
  \end{cases}.
\end{equation}
What can we say about $\theta(T,s)$?

If $T=\infty$, then for any $s<\infty$ it holds that $\theta(\infty,s)=\theta^\star_{-j}$, but for finite $T$ the answer is not clear.
