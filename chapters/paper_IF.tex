\chapter*{If Influence Functions are the Answer, Then What is the Question?}

\section*{Setting}

Consider a prediction task (regression problem) with:
\begin{itemize}
    \item Input space $\X$;
    \item Output space $\Y$;
    \item Training set $\mathcal{D}^n=\{z_i\}_{i=1}^n$ where $z_i=(x_i,y_i)$ for all $i=1,\dots,n$; 
    \\ $X = (x_1,...,x_n)$, $Y=(y_1,...,y_n)$;
    \item Parameter $\theta \in \Theta := \R^d$;
    \item $f(\theta; x)$ estimator of $\Y\mid \X$;
    \item Loss function $\ell:\Y\times\Y\to\R$ (e.g., 
    $\ell(y',y)\mapsto \|y'-y\|^2$).
\end{itemize}

We aim to minimize the training error (empirical risk):
\[
L(\theta;\mathcal{D}^n) 
    = \frac{1}{n}\sum_{i=1}^n \ell\!\big(f(\theta; x_i),\, y_i\big).
\]

Call 
\[
\hat{\theta} \in \arg\min_{\theta\in\Theta} L(\theta; \mathcal{D}^n).
\]

How different is it from
\[
\hat{\theta}_{\varepsilon, -z}
    = \arg\min_{\theta\in\Theta}
        \bigl( L(\theta; \mathcal{D}^n) - \varepsilon\, \ell(f(\theta; x), y)\bigr)
\]

when $\epsilon=1/n$ and $z=z_{\bar i}$ for some $i\in[n]$?\\
To answer this question, we can re-train the whole model on $\mathcal{D}^n\setminus\{z\}$ (Leave-One-Out method), or use influence functions.\\
In the context where the influence functions are well defined, they are a powerful tool. However, when applied to multi-layer perceptrons, for example, their capability of approximating the effect of the LOO decreases drastically.
This paper presents a new point of view: IFs do not approximate the LOO retraining, but instead the effect of another method they present, called PBRF.

\section*{Influence functions}

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence loss difference}
    relative to $\bar{z}$ is
    \[
    \mathcal{Q}(\bar{z};\hat\theta)
        = \left.\frac{d}{d\varepsilon}
            \left[\ell(f(\hat\theta_{\eps,-z},\bar x),\bar y)\right]
        \right|_{\varepsilon=1/n}.
    \]
\end{definition*}


\textit{Interpretation:}  
It measures how much the training error changes when the data point $\bar{z}$ is removed.

\begin{definition*}
    Given $(\bar{x},\bar{y})=\bar{z}\in\mathcal{D}^n$, the \emph{influence function}
    relative to $\bar{z}$ is
    \[
    \mathcal{I}(\bar{z};\hat\theta)
    = \lim_{\varepsilon\to 0}
        \frac{
        \hat{\theta}_{\varepsilon+1/n,-\bar{z}}
        - \hat{\theta}_{1/n,-\bar{z}}
        }{\varepsilon}.
    \]
\end{definition*}


\textit{Interpretation:}  
It represents the direction in which the optimal parameter moves when the
training objective is perturbed by removing $\bar{z}$.\\
Assuming that $L$ is strongly convex in $\theta$, we can rewrite the previous quantities in the closed forms:
\[
\mathcal{I}(z;\hat\theta)
    =  H_{\hat{\theta}}^{-1}\,
      \nabla \ell(f(\hat{\theta};x),y),
\qquad
\mathcal{Q}(z;\hat\theta)
    = \nabla \ell(f(\hat{\theta};x),y)^{\top}
      H_{\hat{\theta}}^{-1}
      \nabla \ell(f(\hat{\theta};x),y),
\]
where $H_{\hat{\theta}}$ is the Hessian of $L$ at $\hat\theta$.
\begin{remark}
  This part is not really clear: referring to the original reference \cite{cook_residuals_1983}, the derivation should be as follow, with our notation.\\
  Let $L_i(\theta) = (L(\hat\theta;\mathcal{D}^n) - 1/n \,\ell(f(\hat\theta;x_i),y_i))$. Assuming that $L$ is twice differentiable, we can use Taylor near $\hat\theta$:
  \begin{equation*}
    L_i(\theta) = L_i(\hat\theta) + (\theta-\hat\theta)^\top L_i'(\hat\theta) + \frac{1}{2}(\theta-\hat\theta)^\top L_i''(\hat\theta) (\theta-\hat\theta) + \mathrm{O}(\|\theta-\hat\theta\|^3)
  \end{equation*}
  Then, by minimizing the difference $L_i(\theta) - L_i(\hat\theta)$ for $\theta\neq\hat\theta$, we get:
  \begin{equation*}
    \theta = \hat\theta - (HL_i(\hat\theta))^{-1}JL_i(\hat\theta).
  \end{equation*}
  From this expression is also clear that the Influence Function estimator represents the parameter after one step of the Newton algorithm starting from $\hat\theta$ trying to get to $\hat\theta_{1/n,-z_i}$. 
\end{remark}

Unfortunately, there is some problems with these formulae:
\begin{enumerate}
    \item the strong convexity is essential. If at a minimum point H has any $0$-eigenvalue, we cannot invert it.
    \item Even when $L$ is strongly convex, the problem is not trivial because computing the inverse of the Hessian and the matrix-vector product are heavy computations.

\end{enumerate}

\textbf{Solution 1.}  
For iHVP, there exist efficient approximations requiring $O(nd)$ flops instead of $O(n^3)$.

\textbf{Solution 2.}  
Change point of view: Influence functions are not approximators of LOO retraining, but instead of the proximal Bregman response function (PBRF).

\section*{Response functions}

We define the response function as:
\[
\hat{r}_z(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \left( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y) \right).
\]
Observe that $\hat{r}_z(\varepsilon)=\hat{\theta}_{\varepsilon,-z}$ and
$\hat{r}_z(0)=\hat{\theta}$.

Since $\hat{r}$ is differentiable at $0$, we can expand with Taylor at first order and we get:
\[
\hat{r}_{z,\mathrm{lin}}(1/n)
  \approx \hat{\theta}
    + \frac1n H_{\hat{\theta}}^{-1}\,
      \nabla\ell(f(\hat{\theta};x),y).
\]
We require $H_{\hat\theta}$ positive definite to invert it; thus $\hat\theta$ must be a minimizer.\\
To compute influence functions for MLPs, we can approximate $H_{\hat{\theta}}$ using the
Gauss--Newton Hessian (GNH) and add damping:
\[
\mathcal{I}^{\dagger}(z;\hat\theta)
  = \left(
        J_{y,\hat{\theta}}^{\top}
        H_{\ell,\hat{\theta}}
        J_{y,\hat{\theta}}
        + \lambda I
    \right)^{-1}
    \nabla\ell(f(\hat{\theta};x),y),
\]
where $J_{y,\hat{\theta}}$ is the Jacobian of  
$F(\theta) = (f(\theta;x_1),\ldots,f(\theta;x_n))$ and $H_{\ell,\hat{\theta}}$ is the hessian of $\ell(f(\theta;x),y)$ in $\theta=\hat\theta$.\\
Note that the damped GNH is always positive definite, as long as $H_{\hat\theta}$ is SPD.\\
We can get the previous formula by linearizing the response function of the regularized loss:
\[
\begin{aligned}
\hat{r}_{z,\mathrm{damp}}(\varepsilon)
  &= \arg\min_{\theta\in\Theta}
     \Big( L(\theta;\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\hat{\theta}\|^2 \Big),\\[4pt]
\hat{r}_{z,\mathrm{damp,lin}}(1/n)
  &\approx \hat{\theta}+\frac1n\mathcal{I}^{\dagger}(z;\hat\theta).
\end{aligned}
\]
Another issue is that, in practice, the parameter we utilise is not a minimizer of $L$. 
Thus, we want to consider a risk for which the early-stopped point $\theta^s$ is optimal:
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  = \frac1n\sum_{i=1}^n
      D_{\ell^{(i)}}\big(f(\theta;x_i), f(\theta^s;x_i)\big),
\]
with $D_{\ell^{(i)}}$ being the Bregman difference:
\[
D_{\ell^{(i)}}(y,y')
 = \ell(y,y_i)
   - \ell(y',y_i)
   - \nabla_1\ell(y',y_i)^\top(y-y').
\]
Intuitively, this quantity measures the difference between the evaluation of $\ell$ on $y$ and the first order Taylor expansion of $\ell$ around $y'$ computed on $y$.
\begin{obs}
  This quantity is always non-negative as long as $\ell$ is convex.
  To exemplify, choose $\ell(y,y')=\|y-y'\|^2/2$. This yields: 
  $$D_{\ell^{(i)}}(y,y')=\|y-y_i\|^2/2-\|y'-y_i\|^2/2-\langle\nabla \|y'-y_i\|^2/2,y-y_i\rangle=\|y-y'\|^2/2.$$
\end{obs}
\vspace{.5cm}\noindent
Consequently, we can define the \emph{Proximal Bregman Response Function} (PBRF) as:
\[
r^b_{z,\mathrm{damp}}(\varepsilon)
  = \arg\min_{\theta\in\Theta}
      \Big(
      \mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
      - \varepsilon\,\ell(f(\theta;x),y)
      + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2
      \Big).
\]
The interesting property of this object is that the linearized PBRF satisfies
\[
r^b_{z,\mathrm{damp,lin}}(1/n)
  = \theta^s + \frac1n\mathcal{I}^{\dagger}(z;\theta^s).
\]

As a consequence, influence functions are \emph{not} approximating LOO retraining under the original
loss $L$. Instead, they approximate the effect of training from $\theta^s$ under the modified objective
\[
\mathcal{L}(\theta;\theta^s,\mathcal{D}^n)
  - \tfrac1n\ell(f(\theta;x),y)
  + \tfrac{\lambda}{2}\|\theta-\theta^s\|^2.
\]
This fact makes PBRF a more suitable benchmark when testing the performances of IFs.\\
Concrete examples show that actually PBRF achieves good results in tasks such as mislabeled example detection, making it a viable alternative to LOO retraining.

\section*{Error decomposition}

The approximation error of influence functions decomposes into:

\begin{itemize}
\item \textbf{Warm-start gap:} LOO starts from a random parameter (cold start), while IF are related to $\theta^s$; we can then converge to another "optimal" point;
\item \textbf{Proximity gap:} the factor $||theta-theta^s||$ induces the warm start not to move far away from $theta^s$;
\item \textbf{Non-convergence gap:} in practice we almost never start from a fully trained network;
\item \textbf{Linearization error:} produced by approximating the Taylor expansion at first order;
\item \textbf{Solver error:} inexact iHVP computation.
\end{itemize}
\begin{remark}
    The PBRF formulation eliminates the first three gaps.
\end{remark}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.9\linewidth]{Err_deco.png}
    \caption{Visual representation of the error defomposition on different datasets and models. The main focus is that the largest components are the first three.}
\end{figure}